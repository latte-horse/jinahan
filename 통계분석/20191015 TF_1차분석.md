# 20191015 TF 로 마무리.



```r
library(dplyr)
library(tidytext)
library(janeaustenr)
library(KoNLP)
library(rJava)
library(data.table)
library(ggplot2)

news_words<-fread(file ='C:/Users/student/Desktop/PJ/통계분석/words.txt',encoding ='UTF-8')
  news_words %>% tail(1)
  
#tail 과 head는 알고 있으면 좋다. 첫부분 6개 뒷부분 6개를 보여준다.

  print(news_words)

news_data_count=news_words %>% select(서울교통공사)%>%

#키값이 '서울교통공사'.

  unnest_tokens_(input="서울교통공
사",output="text")%>%count(text,sort = T)

#'서울교통공사'인 열의 값들을 꺼내서 토큰화 시켜준다. 그리고 토큰들을 다시 'text'에 넣어준다.
그리고 그 토큰들의 값들을 몇번이나 나왔나 숫자를 세어보는 count 함수이고 sort로 내림차순으로 출력을 해준다.

news_data_total=news_words %>% select(서울교통공사)%>%
  unnest_tokens_(input="서울교통공사",output="text")%>%count(text,sort = T)%>%
  summarize(total=sum(n))
  
#이부분은 추후에 교정이 필요한 부분이다. 맨뒤에summarize(total=sum(n))를 빼놓고는 나머지 부분들은 다 같은 코드이다. 
total은 n 즉 키워드를 카운트한 값들의 합이다. 

typeof(news_data_count)
View(TF)
View(news_data_count)
print(news_data_count)

TF=0
for(i in 1:20){
  news_data_count$n[i]
  TF[i]=news_data_count$n[i]/news_data_total
  print(TF)
} 

#내림차순 되어있는 news_data_count$n 값 과 n을 더해준 news_data_total값을 이용해서 TF(term frequency)를 구한다. 
i에는 총 20발의 총알을 쓸 수가 있고 for 문 아래에 있는 총을 이용해 쏜다. 
그리고 TF[i]는 과녁이라서 20발 쏜 결과물이 남는다.

```

> 원래는 TF-IDF를 이용하려 했다. 하지만 TF만 이용하기로 결정을 했다. 
여기에는 몇가지 이유가 있다. 첫 번째로 TF-IDF에서 사용하는 주요 함수중unnest_tokens_은 한글을 분석하는 것에는 잼병이다. 
그 이유는 영어는 각  단어가 모두 띄어쓰기로 쓰여져 있다. 주어, 동사, 분사, 관사. 등등. 
하지만 한글은 지금보는 '한글은'처럼 하나로 붙어있다. 그래서 형태소 분석을 해도 관사가 다붙고
그에 따라 같은단어가 있더라도, 다른 단어로 카운팅이 되어진다.

> 그래서  우리는 khaiii 나 코모란 같은 형태소 분석기가 사용했다. 
여기서 두번째 문제가 생긴다. 형태소 분석기 안에 이미 관사나, 기타 불필요한 의미를 가지고 있는 단어들을 제외시킨다. 
그렇게 되면 의미가 없는 단어보다는 의미가 있는 단어들이 나오게 된다. IDF를 진행하면, 
가장 큰 장점이 바로 빈도는 높으나 뜻이 없는 단어를 제외시켜준다는 것이다. 
하지만 이전 처리 과정에서 이러한 기능을 넣었기에 필요가 없어졌다.
>
> 그래서 빈도수로 1~20위에 위치한 단어들이 키워드가 얼마나 중점적인  의미를 가지는지 표시를 하였다.



```r
결과 값

[[1]]
[1] 0.04296596

[[2]]
[1] 0.02868365

[[3]]
[1] 0.02761247

​	.

​	.

[[18]]
[1] 0.008093311

[[19]]
[1] 0.008093311

[[20]]
[1] 0.007974292
```



